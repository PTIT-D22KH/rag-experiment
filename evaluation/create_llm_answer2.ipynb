{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0befd402-bf20-42dc-87a9-af8ae218478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a0ca0d-a5af-4448-9c5f-1d834a5a2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids1.json', 'rt') as f_in:\n",
    "    documents1 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202cf7d6-2ed2-4879-86cd-b8505b5792a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids2.json', 'rt') as f_in:\n",
    "    documents2 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec54e89-1853-43c4-aea9-b207a7c58a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids3.json', 'rt') as f_in:\n",
    "    documents3 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4810ac-524b-4210-b6ad-6ba8e2deb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids4.json', 'rt') as f_in:\n",
    "    documents4 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b63519-50ed-4e09-b4b8-325d3fab43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids5.json', 'rt') as f_in:\n",
    "    documents5 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40125567-5f12-4004-9a81-3879b2abd294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_documents(base_path, num_files):\n",
    "    documents = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = f'{base_path}/documents-with-ids{i}.json'\n",
    "        with open(file_path, 'rt') as f_in:\n",
    "            documents.extend(json.load(f_in))\n",
    "    return documents\n",
    "base_path = '../data/vietnamese_rag'\n",
    "num_files = 5\n",
    "documents = load_documents(base_path, num_files)\n",
    "df_ground_truth = pd.read_csv('../data/vietnamese_rag/ground_truth_data/ground_truth_data.csv')\n",
    "\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "doc_idx = {d['id']: d for d in documents}\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2aefd1-fd0b-4175-8341-ad873eb3a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'vietnamese-questions'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"group\": {\"type\": \"keyword\"},\n",
    "            \"context\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"context_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"answer_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"question_context_answer_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"vietnamese-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a14048-4b72-4bf0-9c32-f775a829f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 6089/6089 [02:24<00:00, 42.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_vectors(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def process_documents(documents, index_name, es_client):\n",
    "    full_documents = []\n",
    "    for i in range(1, 6):\n",
    "        if i == 1:\n",
    "            data = documents1.copy()\n",
    "        elif i == 2:\n",
    "            data = documents2.copy()\n",
    "        elif i == 3:\n",
    "            data = documents3.copy()\n",
    "        elif i == 4:\n",
    "            data = documents4.copy()\n",
    "        elif i == 5:\n",
    "            data = documents5.copy()\n",
    "        document_qta_vector_list = load_vectors(f'../data/vietnamese_rag/question_context_answer_vector_pickle/question_context_answer_vector{i}.pkl')\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            data[j]['question_context_answer_vector'] = document_qta_vector_list[j]['question_context_answer_vector']\n",
    "        full_documents.extend(data)\n",
    "    for doc in tqdm(full_documents):\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "process_documents(documents, index_name, es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3c5b76-59a6-4d5e-b962-493dfa9290b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_knn(field, vector, group):\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"group\": group\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"group\", \"context\", \"question\", \"answer\", \"id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs\n",
    "def question_context_answer_vector_knn(q):\n",
    "    question = q['question']\n",
    "    group = q['Group']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "\n",
    "    return elastic_search_knn('question_context_answer_vector', v_q, group)\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're an assistant working in customer service. Your job is to provide answers to users' questions. Answer the QUESTION based on the CONTEXT from the documents database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Provide answer in Vietnamese , in normal text form, not using any markdown form, no need to rewrite the question and make sure that is an answer, not listing questions. Also make sure that the answer provides most information from the CONTEXT as possible .\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"group: {doc['group']}\\nquestion: {doc['question']}\\nanswer: {doc['answer']}\\ncontext: {doc['context'][:1000]}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "client =  Groq(api_key = os.environ['GROQ_API_KEY4'])\n",
    "def llm(prompt, model = 'mixtral-8x7b-32768'):\n",
    "    retries = 5\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model= 'llama3-8b-8192',\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            json_response = response.choices[0].message.content\n",
    "            return json_response\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code == 429:  # Rate limit error\n",
    "                retry_after = float(e.response.json()['error']['message'].split('in ')[-1].split('s')[0])\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            if i < retries - 1:\n",
    "                time.sleep(2 ** i)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "# previously: rag(query: str) -> str\n",
    "def rag(query: dict, model='mixtral-8x7b-32768') -> str:\n",
    "    search_results = question_context_answer_vector_knn(query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer\n",
    "pool = ThreadPoolExecutor(max_workers=6)\n",
    "def map_progress(pool, seq, f):\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "\n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_record(rec):\n",
    "    model = 'mixtral-8x7b-32768'\n",
    "    answer_llm = rag(rec, model = model)\n",
    "    doc_id = rec['document']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    answer_orig = original_doc['answer']\n",
    "\n",
    "    return {\n",
    "        'answer_llm': answer_llm,\n",
    "        'answer_orig': answer_orig,\n",
    "        'document': doc_id,\n",
    "        'question': rec['question'],\n",
    "        'group': rec['Group'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2431cb75-1ebc-460d-8f44-17f7abfd14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_current = ground_truth[-1215*2:-1215]\n",
    "len(documents_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807b2c30-60ca-4f11-a139-4157913f7a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 0 15\n",
      "83 15 30\n",
      "84 30 45\n",
      "85 45 60\n",
      "86 60 75\n",
      "87 75 90\n",
      "88 90 105\n",
      "89 105 120\n",
      "90 120 135\n",
      "91 135 150\n",
      "92 150 165\n",
      "93 165 180\n",
      "94 180 195\n",
      "95 195 210\n",
      "96 210 225\n",
      "97 225 240\n",
      "98 240 255\n",
      "99 255 270\n",
      "100 270 285\n",
      "101 285 300\n",
      "102 300 315\n",
      "103 315 330\n",
      "104 330 345\n",
      "105 345 360\n",
      "106 360 375\n",
      "107 375 390\n",
      "108 390 405\n",
      "109 405 420\n",
      "110 420 435\n",
      "111 435 450\n",
      "112 450 465\n",
      "113 465 480\n",
      "114 480 495\n",
      "115 495 510\n",
      "116 510 525\n",
      "117 525 540\n",
      "118 540 555\n",
      "119 555 570\n",
      "120 570 585\n",
      "121 585 600\n",
      "122 600 615\n",
      "123 615 630\n",
      "124 630 645\n",
      "125 645 660\n",
      "126 660 675\n",
      "127 675 690\n",
      "128 690 705\n",
      "129 705 720\n",
      "130 720 735\n",
      "131 735 750\n",
      "132 750 765\n",
      "133 765 780\n",
      "134 780 795\n",
      "135 795 810\n",
      "136 810 825\n",
      "137 825 840\n",
      "138 840 855\n",
      "139 855 870\n",
      "140 870 885\n",
      "141 885 900\n",
      "142 900 915\n",
      "143 915 930\n",
      "144 930 945\n",
      "145 945 960\n",
      "146 960 975\n",
      "147 975 990\n",
      "148 990 1005\n",
      "149 1005 1020\n",
      "150 1020 1035\n",
      "151 1035 1050\n",
      "152 1050 1065\n",
      "153 1065 1080\n",
      "154 1080 1095\n",
      "155 1095 1110\n",
      "156 1110 1125\n",
      "157 1125 1140\n",
      "158 1140 1155\n",
      "159 1155 1170\n",
      "160 1170 1185\n",
      "161 1185 1200\n",
      "162 1200 1216\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 15\n",
    "start_chunk = 0 # Starting chunk index\n",
    "end_chunk = (len(documents_current) // chunk_size)   # Ending chunk index\n",
    "# print(end_chunk)\n",
    "for i in range(start_chunk, end_chunk):\n",
    "    results = []\n",
    "    chunk_start = i * chunk_size\n",
    "    chunk_end = chunk_start + chunk_size\n",
    "    if (i == end_chunk - 1):\n",
    "        chunk_end = chunk_start + chunk_size + 1\n",
    "    print(i + 82, chunk_start, chunk_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a81d5-86c2-4948-bcb9-4dad1adeb357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:05<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last82.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [01:41<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last83.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:55<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last84.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:16<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last85.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:04<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last86.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:31<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last87.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:37<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last88.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:37<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last89.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [01:41<00:00,  6.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last90.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [01:34<00:00,  6.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last91.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:50<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last92.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:04<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last93.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:08<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 12 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last94.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:30<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 13 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last95.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [00:36<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 14 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last96.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [01:05<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 15 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last97.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 15/15 [01:25<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 16 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last98.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                      | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "chunk_size = 15\n",
    "start_chunk = 0 # Starting chunk index\n",
    "end_chunk = (len(documents_current) // chunk_size)   # Ending chunk index\n",
    "# print(end_chunk)\n",
    "for i in range(start_chunk, end_chunk):\n",
    "    results = []\n",
    "    chunk_start = i * chunk_size\n",
    "    chunk_end = chunk_start + chunk_size\n",
    "    if (i == end_chunk - 1):\n",
    "        chunk_end = chunk_start + chunk_size + 1\n",
    "    # print(i + 1, chunk_start, chunk_end)\n",
    "    chunk = documents_current[chunk_start:chunk_end]\n",
    "\n",
    "    # Use map_progress to process documents\n",
    "    processed_results = map_progress(pool, chunk, process_record)\n",
    "    results.extend(processed_results)\n",
    "    # # Store the results incrementally\n",
    "    # for result in processed_results:\n",
    "    #     if result is not None:\n",
    "    #         doc_id, questions = result\n",
    "    #         results[doc_id] = questions\n",
    "\n",
    "    # Save the results to a file\n",
    "    file_name = f'../data/vietnamese_rag/llm_answer/llm_answer_last{i + 82}.pkl'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    # Print out the results\n",
    "    print(f\"Chunk {i} processed and saved to {file_name}\")\n",
    "    # print(results)\n",
    "\n",
    "    # Wait for 1 minute to reset rate limit\n",
    "    # time.sleep(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486da5b7-db80-427a-b93c-d628c237c241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f92350-4adc-4a95-9772-76ee116bc33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
