{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0befd402-bf20-42dc-87a9-af8ae218478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a0ca0d-a5af-4448-9c5f-1d834a5a2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids1.json', 'rt') as f_in:\n",
    "    documents1 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202cf7d6-2ed2-4879-86cd-b8505b5792a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids2.json', 'rt') as f_in:\n",
    "    documents2 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec54e89-1853-43c4-aea9-b207a7c58a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids3.json', 'rt') as f_in:\n",
    "    documents3 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4810ac-524b-4210-b6ad-6ba8e2deb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids4.json', 'rt') as f_in:\n",
    "    documents4 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b63519-50ed-4e09-b4b8-325d3fab43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vietnamese_rag/documents-with-ids5.json', 'rt') as f_in:\n",
    "    documents5 = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40125567-5f12-4004-9a81-3879b2abd294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_documents(base_path, num_files):\n",
    "    documents = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        file_path = f'{base_path}/documents-with-ids{i}.json'\n",
    "        with open(file_path, 'rt') as f_in:\n",
    "            documents.extend(json.load(f_in))\n",
    "    return documents\n",
    "base_path = '../data/vietnamese_rag'\n",
    "num_files = 5\n",
    "documents = load_documents(base_path, num_files)\n",
    "df_ground_truth = pd.read_csv('../data/vietnamese_rag/ground_truth_data/ground_truth_data.csv')\n",
    "\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "doc_idx = {d['id']: d for d in documents}\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2aefd1-fd0b-4175-8341-ad873eb3a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'vietnamese-questions'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"group\": {\"type\": \"keyword\"},\n",
    "            \"context\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"context_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"answer_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"question_context_answer_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"vietnamese-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a14048-4b72-4bf0-9c32-f775a829f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 6089/6089 [02:24<00:00, 42.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_vectors(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def process_documents(documents, index_name, es_client):\n",
    "    full_documents = []\n",
    "    for i in range(1, 6):\n",
    "        if i == 1:\n",
    "            data = documents1.copy()\n",
    "        elif i == 2:\n",
    "            data = documents2.copy()\n",
    "        elif i == 3:\n",
    "            data = documents3.copy()\n",
    "        elif i == 4:\n",
    "            data = documents4.copy()\n",
    "        elif i == 5:\n",
    "            data = documents5.copy()\n",
    "        document_qta_vector_list = load_vectors(f'../data/vietnamese_rag/question_context_answer_vector_pickle/question_context_answer_vector{i}.pkl')\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            data[j]['question_context_answer_vector'] = document_qta_vector_list[j]['question_context_answer_vector']\n",
    "        full_documents.extend(data)\n",
    "    for doc in tqdm(full_documents):\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "process_documents(documents, index_name, es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3c5b76-59a6-4d5e-b962-493dfa9290b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_knn(field, vector, group):\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"group\": group\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"group\", \"context\", \"question\", \"answer\", \"id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs\n",
    "def question_context_answer_vector_knn(q):\n",
    "    question = q['question']\n",
    "    group = q['Group']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "\n",
    "    return elastic_search_knn('question_context_answer_vector', v_q, group)\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're an assistant working in customer service. Your job is to provide answers to users' questions. Answer the QUESTION based on the CONTEXT from the documents database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Provide answer in Vietnamese , in normal text form, not using any markdown form, no need to rewrite the question and make sure that is an answer, not listing questions. Also make sure that the answer provides most information from the CONTEXT as possible .\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"group: {doc['group']}\\nquestion: {doc['question']}\\nanswer: {doc['answer']}\\ncontext: {doc['context'][:1000]}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "client =  Groq(api_key = os.environ['GROQ_API_KEY4'])\n",
    "def llm(prompt, model = 'mixtral-8x7b-32768'):\n",
    "    retries = 5\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model= 'llama3-8b-8192',\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            json_response = response.choices[0].message.content\n",
    "            return json_response\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code == 429:  # Rate limit error\n",
    "                retry_after = float(e.response.json()['error']['message'].split('in ')[-1].split('s')[0])\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            if i < retries - 1:\n",
    "                time.sleep(2 ** i)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "# previously: rag(query: str) -> str\n",
    "def rag(query: dict, model='mixtral-8x7b-32768') -> str:\n",
    "    search_results = question_context_answer_vector_knn(query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer\n",
    "pool = ThreadPoolExecutor(max_workers=6)\n",
    "def map_progress(pool, seq, f):\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "\n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_record(rec):\n",
    "    model = 'mixtral-8x7b-32768'\n",
    "    answer_llm = rag(rec, model = model)\n",
    "    doc_id = rec['document']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    answer_orig = original_doc['answer']\n",
    "\n",
    "    return {\n",
    "        'answer_llm': answer_llm,\n",
    "        'answer_orig': answer_orig,\n",
    "        'document': doc_id,\n",
    "        'question': rec['question'],\n",
    "        'group': rec['Group'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2431cb75-1ebc-460d-8f44-17f7abfd14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_current = ground_truth[-1215*2:-1215]\n",
    "len(documents_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3a81d5-86c2-4948-bcb9-4dad1adeb357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed and saved to ../data/vietnamese_rag/llm_answer/llm_answer_last1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████▏                    | 9/15 [01:12<00:48,  8.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m chunk \u001b[38;5;241m=\u001b[39m documents_current[chunk_start:chunk_end]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Use map_progress to process documents\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m processed_results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results\u001b[38;5;241m.\u001b[39mextend(processed_results)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# # Store the results incrementally\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# for result in processed_results:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     if result is not None:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the results to a file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 95\u001b[0m, in \u001b[0;36mmap_progress\u001b[0;34m(pool, seq, f)\u001b[0m\n\u001b[1;32m     92\u001b[0m         futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m---> 95\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chunk_size = 15\n",
    "start_chunk = 0 # Starting chunk index\n",
    "end_chunk = (len(documents_current) // chunk_size)   # Ending chunk index\n",
    "# print(end_chunk)\n",
    "for i in range(start_chunk, end_chunk):\n",
    "    results = []\n",
    "    chunk_start = i * chunk_size\n",
    "    chunk_end = chunk_start + chunk_size\n",
    "    if (i == end_chunk - 1):\n",
    "        chunk_end = chunk_start + chunk_size + 1\n",
    "    # print(i + 1, chunk_start, chunk_end)\n",
    "    chunk = documents_current[chunk_start:chunk_end]\n",
    "\n",
    "    # Use map_progress to process documents\n",
    "    processed_results = map_progress(pool, chunk, process_record)\n",
    "    results.extend(processed_results)\n",
    "    # # Store the results incrementally\n",
    "    # for result in processed_results:\n",
    "    #     if result is not None:\n",
    "    #         doc_id, questions = result\n",
    "    #         results[doc_id] = questions\n",
    "\n",
    "    # Save the results to a file\n",
    "    file_name = f'../data/vietnamese_rag/llm_answer/llm_answer_last{i + 1}.pkl'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    # Print out the results\n",
    "    print(f\"Chunk {i} processed and saved to {file_name}\")\n",
    "    # print(results)\n",
    "\n",
    "    # Wait for 1 minute to reset rate limit\n",
    "    # time.sleep(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486da5b7-db80-427a-b93c-d628c237c241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f92350-4adc-4a95-9772-76ee116bc33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
