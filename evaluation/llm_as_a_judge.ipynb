{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb1f13ea-c16c-450c-8b78-6757d1f7be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53284ee3-7ed6-42c9-9a50-8bafc94756e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d22ef0-0bb1-4aca-94a9-e5d7d61b9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client =  Groq(api_key = os.environ['GROQ_API_KEY1'])\n",
    "def llm(prompt, model = 'mixtral-8x7b-32768'):\n",
    "    retries = 5\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model= 'llama3-8b-8192',\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            json_response = response.choices[0].message.content\n",
    "            return json_response\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code == 429:  # Rate limit error\n",
    "                retry_after = float(e.response.json()['error']['message'].split('in ')[-1].split('s')[0])\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            if i < retries - 1:\n",
    "                time.sleep(2 ** i)  # Exponential backoff\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "542bb9fc-0911-435c-b395-2f0d809b6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(1,83):\n",
    "    \n",
    "    with open(f\"../data/vietnamese_rag/llm_answer/llm_answer{i}.pkl\", 'rb') as file:\n",
    "        r = pickle.load(file)\n",
    "    results.extend(r)\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07c148bd-c105-45f9-8b0b-e64d80ae13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1217 entries, 0 to 1216\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   answer_llm   1217 non-null   object\n",
      " 1   answer_orig  1217 non-null   object\n",
      " 2   document     1217 non-null   object\n",
      " 3   question     1217 non-null   object\n",
      " 4   group        1217 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 47.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01f2506e-1596-4b59-897c-a8ca9e23345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/vietnamese_rag/llm_answer/llm_answer1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099019c8-74dd-4b5a-8d58-cdac59b6289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 15/15 [00:07<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 417 (char 445)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 593 (char 628)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 414 (char 442)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 418 (char 446)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 392 (char 420)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 321 (char 347)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 417 (char 450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 15/15 [00:19<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 395 (char 423)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 413 (char 441)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 339 (char 367)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 616 (char 642)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 321 (char 349)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 269 (char 297)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 444 (char 476)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 454 (char 489)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 306 (char 332)\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to fix JSON string: Expecting value: line 1 column 1 (char 0)\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to fix JSON string: Expecting value: line 1 column 1 (char 0)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 498 (char 526)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 15/15 [00:39<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 306 (char 334)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 498 (char 533)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 323 (char 358)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 411 (char 446)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 318 (char 346)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 428 (char 456)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 364 (char 392)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 269 (char 297)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 392 (char 427)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 365 (char 391)\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to fix JSON string: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██▌                | 2/15 [00:05<00:36,  2.84s/it]"
     ]
    }
   ],
   "source": [
    "for i in range(2, 83):\n",
    "    evaluations = []\n",
    "    results = []\n",
    "    json_evaluations = []\n",
    "    \n",
    "    # Load the pickle file\n",
    "    with open(f\"../data/vietnamese_rag/llm_answer/llm_answer{i}.pkl\", 'rb') as file:\n",
    "        r = pickle.load(file)\n",
    "    results.extend(r)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    samples = df.to_dict(orient='records')\n",
    "    \n",
    "    # Generate evaluations\n",
    "    for record in tqdm(samples):\n",
    "        prompt = prompt1_template.format(**record)\n",
    "        evaluation = llm(prompt)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    # Parse evaluations\n",
    "    for j, str_eval in enumerate(evaluations):\n",
    "        try:\n",
    "            json_eval = json.loads(str_eval)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "            # Attempt to fix the JSON string\n",
    "            try:\n",
    "                str_eval = str_eval.rstrip('}') + '}'  # Ensure it ends with a closing brace\n",
    "                json_eval = json.loads(str_eval)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to fix JSON string: {e}\")\n",
    "                continue  # Skip this evaluation if it cannot be fixed\n",
    "        json_evaluations.append(json_eval)\n",
    "    \n",
    "    # Save evaluations to CSV\n",
    "    if json_evaluations:\n",
    "        df_evaluations = pd.DataFrame(json_evaluations)\n",
    "        df_evaluations.to_csv(f'../data/vietnamese_rag/evaluations_aqa/evaluations-aqa{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "732accb0-12df-46ef-ad49-1711255c942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 15/15 [00:06<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    evaluations = []\n",
    "    results = []\n",
    "    json_evaluations = []\n",
    "    with open(f\"../data/vietnamese_rag/llm_answer/llm_answer1.pkl\", 'rb') as file:\n",
    "        r = pickle.load(file)\n",
    "    results.extend(r)\n",
    "    df = pd.DataFrame(results)\n",
    "    samples = df.to_dict(orient='records')\n",
    "    for record in tqdm(samples):\n",
    "        prompt = prompt1_template.format(**record)\n",
    "        evaluation = llm(prompt)\n",
    "        evaluations.append(evaluation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d01a3c2f-4846-4cba-a020-559c9de11291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 403 (char 431)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 468 (char 496)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 583 (char 611)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 620 (char 648)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 515 (char 550)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 512 (char 540)\n",
      "JSONDecodeError: Expecting ',' delimiter: line 3 column 402 (char 430)\n"
     ]
    }
   ],
   "source": [
    "for i, str_eval in enumerate(evaluations):\n",
    "    # print(str_eval)\n",
    "    try:\n",
    "        json_eval = json.loads(str_eval)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        str_eval = str_eval + \"}\"\n",
    "        json_eval = json.loads(str_eval)\n",
    "        # print(\"After fix: \", json_eval)\n",
    "    json_evaluations.append(json_eval)\n",
    "df_evaluations = pd.DataFrame(json_evaluations)\n",
    "df_evaluations.to_csv(f'../data/vietnamese_rag/evaluations_aqa/evaluations-aqa1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c948fe7-e430-4392-83fa-fa555e105875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially corresponds to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is partially summarized f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially corresponds to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is partially summarized f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is a direct and literal r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is identical to the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer contains a specific event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is highly similar to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is highly relevant to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially addresses the q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>Although the generated answer has minor discre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially addresses the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer accurately restates the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer closely matches the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially addresses the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is identical to the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is relevant because it pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Relevance                                        Explanation\n",
       "0   PARTLY_RELEVANT  The generated answer partially corresponds to ...\n",
       "1          RELEVANT  The generated answer is partially summarized f...\n",
       "2   PARTLY_RELEVANT  The generated answer partially corresponds to ...\n",
       "3          RELEVANT  The generated answer is partially summarized f...\n",
       "4          RELEVANT  The generated answer is a direct and literal r...\n",
       "5          RELEVANT  The generated answer is identical to the origi...\n",
       "6          RELEVANT  The generated answer contains a specific event...\n",
       "7          RELEVANT  The generated answer is highly similar to the ...\n",
       "8          RELEVANT  The generated answer is highly relevant to the...\n",
       "9   PARTLY_RELEVANT  The generated answer partially addresses the q...\n",
       "10         RELEVANT  Although the generated answer has minor discre...\n",
       "11  PARTLY_RELEVANT  The generated answer partially addresses the o...\n",
       "12         RELEVANT  The generated answer accurately restates the o...\n",
       "13         RELEVANT  The generated answer closely matches the origi...\n",
       "14  PARTLY_RELEVANT  The generated answer partially addresses the o...\n",
       "15         RELEVANT  The generated answer is identical to the origi...\n",
       "16         RELEVANT  The generated answer is relevant because it pr..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cefb6fe-4a56-4284-b178-eee4ca24bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 0: {\"Relevance\": \"PARTLY_RELEVANT\", \"Explanation\": \"The generated answer partially corresponds to the original answer, as it describes Minh Tú's experience in the catwalk challenge on Asia's Next Top Model, which is similar to the context in the original answer. However, the generated answer focuses more on the outcome (her photo being considered too sexy) and doesn't fully capture the original answer's description of Minh Tú overcoming fear to complete the challenge successfully. The relevance is partly relevant because it shares some similar context, but crucial aspects of the original answer are missing.\"}\n",
      "         Relevance                                        Explanation\n",
      "0  PARTLY_RELEVANT  The generated answer partially corresponds to ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "evaluations = [\n",
    "    '{\"Relevance\": \"PARTLY_RELEVANT\", \"Explanation\": \"The generated answer partially corresponds to the original answer, as it describes Minh Tú\\'s experience in the catwalk challenge on Asia\\'s Next Top Model, which is similar to the context in the original answer. However, the generated answer focuses more on the outcome (her photo being considered too sexy) and doesn\\'t fully capture the original answer\\'s description of Minh Tú overcoming fear to complete the challenge successfully. The relevance is partly relevant because it shares some similar context, but crucial aspects of the original answer are missing.\"}'\n",
    "]\n",
    "\n",
    "json_evaluations = []\n",
    "\n",
    "for i, str_eval in enumerate(evaluations):\n",
    "    print(f\"Evaluation {i}: {str_eval}\")\n",
    "    try:\n",
    "        json_eval = json.loads(str_eval)\n",
    "        json_evaluations.append(json_eval)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}\")\n",
    "        # Optionally, handle the error or log it for further investigation\n",
    "\n",
    "# If no errors, create DataFrame\n",
    "if json_evaluations:\n",
    "    import pandas as pd\n",
    "    df_evaluations = pd.DataFrame(json_evaluations)\n",
    "    print(df_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9692661-66bd-443b-b6ae-d992e78807bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer partially corresponds to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Relevance                                        Explanation\n",
       "0  PARTLY_RELEVANT  The generated answer partially corresponds to ..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38c692-0a3f-4b7c-ab70-87b5082f6a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
